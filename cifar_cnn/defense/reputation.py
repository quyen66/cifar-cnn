"""
Reputation System - Asymmetric EMA
===================================
H·ªá th·ªëng danh ti·∫øng ƒë√°nh gi√° ƒë·ªô tin c·∫≠y c·ªßa client theo th·ªùi gian.

Components:
- Consistency Score (Immediate + History)
- Participation Score (Cosine Similarity)
- Asymmetric EMA Update (Penalize Fast, Reward Slow)
- Floor Lifting Mechanism

Author: Week 3 Implementation
"""

import numpy as np
from typing import Dict, List, Optional
from collections import deque


class ReputationSystem:
    """
    H·ªá th·ªëng danh ti·∫øng b·∫•t ƒë·ªëi x·ª©ng v·ªõi floor lifting.
    
    Tri·∫øt l√Ω:
    - Tr·ª´ng ph·∫°t nhanh (Œ±_down = 0.8) khi h√†nh vi x·∫•u
    - Th∆∞·ªüng ch·∫≠m (Œ±_up = 0.3) khi h√†nh vi t·ªët
    - C∆° h·ªôi th·ª© hai cho client b·ªã ƒë√°nh gi√° sai (floor lifting)
    """
    
    def __init__(self,
                 alpha_down: float = 0.8,
                 alpha_up: float = 0.3,
                 floor_threshold: float = 0.15,
                 floor_target: float = 0.3,
                 floor_patience: int = 5,
                 history_window: int = 5):
        """
        Args:
            alpha_down: EMA rate khi gi·∫£m danh ti·∫øng (default=0.8, nhanh)
            alpha_up: EMA rate khi tƒÉng danh ti·∫øng (default=0.3, ch·∫≠m)
            floor_threshold: Ng∆∞·ª°ng ƒë·ªÉ trigger floor lifting (default=0.15)
            floor_target: Target reputation khi lift (default=0.3)
            floor_patience: S·ªë rounds good behavior ƒë·ªÉ lift (default=5)
            history_window: S·ªë rounds l∆∞u l·ªãch s·ª≠ (default=5)
        """
        self.alpha_down = alpha_down
        self.alpha_up = alpha_up
        self.floor_threshold = floor_threshold
        self.floor_target = floor_target
        self.floor_patience = floor_patience
        self.history_window = history_window
        
        # Storage
        self.reputations: Dict[int, float] = {}  # client_id -> reputation
        self.history: Dict[int, deque] = {}  # client_id -> deque of past gradients
        self.floor_counters: Dict[int, int] = {}  # client_id -> good behavior count
        
        print(f"‚úÖ ReputationSystem initialized:")
        print(f"   - Alpha down (penalize): {alpha_down}")
        print(f"   - Alpha up (reward): {alpha_up}")
        print(f"   - Floor lifting: {floor_threshold} ‚Üí {floor_target} (patience={floor_patience})")
        print(f"   - History window: {history_window} rounds")
    
    def initialize_client(self, client_id: int, initial_reputation: float = 0.8):
        """Initialize m·ªôt client m·ªõi."""
        if client_id not in self.reputations:
            self.reputations[client_id] = initial_reputation
            self.history[client_id] = deque(maxlen=self.history_window)
            self.floor_counters[client_id] = 0
    
    def update(self,
               client_id: int,
               gradient: np.ndarray,
               grad_median: np.ndarray,
               was_flagged: bool,
               current_round: int) -> float:
        """
        C·∫≠p nh·∫≠t reputation cho m·ªôt client.
        
        Args:
            client_id: ID c·ªßa client
            gradient: Gradient hi·ªán t·∫°i
            grad_median: Gradient median (reference)
            was_flagged: Client c√≥ b·ªã ƒë√°nh d·∫•u kh√¥ng
            current_round: Round hi·ªán t·∫°i
        
        Returns:
            Reputation m·ªõi
        """
        # Initialize n·∫øu ch∆∞a c√≥
        self.initialize_client(client_id)
        
        # T√≠nh raw reputation score
        raw_score = self._compute_raw_score(
            client_id, gradient, grad_median
        )
        
        # Update v·ªõi Asymmetric EMA
        old_rep = self.reputations[client_id]
        new_rep = self._asymmetric_ema_update(
            old_rep, raw_score, was_flagged
        )
        
        # Update storage
        self.reputations[client_id] = new_rep
        self.history[client_id].append(gradient.flatten())
        
        # Check floor lifting
        if new_rep <= self.floor_threshold:
            self._check_floor_lifting(client_id, was_flagged)
        else:
            self.floor_counters[client_id] = 0
        
        return new_rep
    
    def _compute_raw_score(self,
                          client_id: int,
                          gradient: np.ndarray,
                          grad_median: np.ndarray) -> float:
        """
        T√≠nh raw reputation score d·ª±a tr√™n Consistency v√† Participation.
        
        Formula (theo main.pdf):
            r(i,t) = 0.5 √ó C(i,t) + 0.5 √ó P(i,t)
        
        C(i,t) = Consistency = 0.5 √ó (0.6 √ó C_immediate + 0.4 √ó C_history + 1)
        P(i,t) = Participation = 0.5 √ó (cosine_sim + 1)
        
        FIX: Better handling of history to differentiate mixed behavior
        """
        g_flat = gradient.flatten()
        
        # === Participation Score (Cosine Similarity) ===
        cosine_sim = np.dot(g_flat, grad_median) / (
            np.linalg.norm(g_flat) * np.linalg.norm(grad_median) + 1e-10
        )
        participation = 0.5 * (cosine_sim + 1)  # Scale to [0, 1]
        
        # === Consistency Score ===
        # Immediate consistency (v·ªõi gradient hi·ªán t·∫°i)
        c_immediate = cosine_sim  # Reuse cosine similarity
        
        # Historical consistency (v·ªõi past gradients)
        c_history = 0.0
        if len(self.history[client_id]) > 0:
            # FIX: T√≠nh avg cosine sim v·ªõi past gradients
            # V√† penalty n·∫øu variance cao (inconsistent)
            past_sims = []
            for past_grad in self.history[client_id]:
                sim = np.dot(g_flat, past_grad) / (
                    np.linalg.norm(g_flat) * np.linalg.norm(past_grad) + 1e-10
                )
                past_sims.append(sim)
            
            # Mean similarity
            mean_sim = np.mean(past_sims)
            
            # Variance penalty (high variance = inconsistent)
            # FINAL FIX: Reduce max penalty from 0.3 to 0.2
            var_sim = np.var(past_sims)
            variance_penalty = np.clip(var_sim, 0, 0.2)  # Was 0.3, now 0.2
            
            c_history = mean_sim - variance_penalty
        
        # Combine consistency
        # FIX: Increase history weight to better track behavior
        consistency = 0.5 * (0.5 * c_immediate + 0.5 * c_history + 1)
        
        # === Final Raw Score ===
        raw_score = 0.5 * consistency + 0.5 * participation
        
        return np.clip(raw_score, 0.0, 1.0)
    
    def _asymmetric_ema_update(self,
                               old_rep: float,
                               raw_score: float,
                               was_flagged: bool) -> float:
        """
        Asymmetric EMA update.
        
        Formula (main.pdf):
            R(i,t) = (1-Œ±) √ó R(i,t-1) + Œ± √ó r(i,t)
        
        V·ªõi:
            Œ± = Œ±_down (0.8) n·∫øu gi·∫£m
            Œ± = Œ±_up (0.3) n·∫øu tƒÉng
        
        FIX FINAL: Reduce penalty even more for mixed behavior
            N·∫øu was_flagged: raw_score √ó 0.8 (was 0.7, now 0.8)
        """
        # FINAL FIX: Even less harsh penalty for mixed clients
        if was_flagged:
            raw_score = raw_score * 0.8  # Changed from 0.7 to 0.8
        
        # Asymmetric alpha
        if raw_score < old_rep:
            # Gi·∫£m nhanh
            alpha = self.alpha_down
        else:
            # TƒÉng ch·∫≠m
            alpha = self.alpha_up
        
        # EMA update
        new_rep = (1 - alpha) * old_rep + alpha * raw_score
        
        return np.clip(new_rep, 0.0, 1.0)
    
    def _check_floor_lifting(self, client_id: int, was_flagged: bool):
        """
        C∆° ch·∫ø floor lifting - cho c∆° h·ªôi th·ª© hai.
        
        Logic:
        - N·∫øu reputation ‚â§ floor_threshold trong patience rounds
        - V√Ä h√†nh vi t·ªët (kh√¥ng b·ªã flag)
        - TH√å n√¢ng l√™n floor_target
        """
        if not was_flagged:
            # Good behavior
            self.floor_counters[client_id] += 1
            
            if self.floor_counters[client_id] >= self.floor_patience:
                # Lift floor!
                old_rep = self.reputations[client_id]
                self.reputations[client_id] = self.floor_target
                self.floor_counters[client_id] = 0
                
                print(f"   üîº Floor Lifting: Client {client_id} "
                      f"{old_rep:.3f} ‚Üí {self.floor_target:.3f} "
                      f"(after {self.floor_patience} good rounds)")
        else:
            # Bad behavior - reset counter
            self.floor_counters[client_id] = 0
    
    def get_reputation(self, client_id: int) -> float:
        """L·∫•y reputation hi·ªán t·∫°i."""
        if client_id not in self.reputations:
            self.initialize_client(client_id)
        return self.reputations[client_id]
    
    def get_all_reputations(self) -> Dict[int, float]:
        """L·∫•y t·∫•t c·∫£ reputations."""
        return self.reputations.copy()
    
    def get_stats(self) -> Dict:
        """Get statistics."""
        if not self.reputations:
            return {}
        
        reps = list(self.reputations.values())
        return {
            'num_clients': len(reps),
            'mean_reputation': np.mean(reps),
            'min_reputation': np.min(reps),
            'max_reputation': np.max(reps),
            'below_floor': sum(1 for r in reps if r <= self.floor_threshold)
        }