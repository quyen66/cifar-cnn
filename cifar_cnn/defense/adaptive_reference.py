"""
Adaptive Hybrid Reference Tracker (NEW FILE)
=============================================
Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ FPR cao khi attack corrupt current-round reference.

NGUY√äN L√ù:
- Khi H score th·∫•p (√≠t attack) ‚Üí d√πng current-round median (ƒë√°ng tin)
- Khi H score cao (nhi·ªÅu attack) ‚Üí d√πng historical momentum (kh√¥ng b·ªã corrupt)

C√îNG TH·ª®C:
    Œ± = clip(Œ±_base + Œ±_h_mult √ó H, Œ±_min, Œ±_max)
    reference = Œ± √ó historical_momentum + (1-Œ±) √ó current_median

V√ç D·ª§:
    H = 0.1 (IID, √≠t attack)  ‚Üí Œ± ‚âà 0.25 ‚Üí 75% current, 25% historical
    H = 0.6 (high attack)     ‚Üí Œ± ‚âà 0.55 ‚Üí 45% current, 55% historical
    H = 0.9 (severe attack)   ‚Üí Œ± ‚âà 0.70 ‚Üí 30% current, 70% historical

T√çCH H·ª¢P:
    1. Copy file n√†y v√†o cifar_cnn/defense/
    2. Import v√†o __init__.py
    3. S·ª≠a server_app.py theo h∆∞·ªõng d·∫´n
"""

import numpy as np
from typing import Optional, Dict, List


class AdaptiveReferenceTracker:
    """
    Tracks historical gradient momentum and computes adaptive reference
    for Layer 2 detection.
    
    Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ: Khi >30% clients l√† attackers, current-round median
    b·ªã corrupt ‚Üí benign clients fail cosine check ‚Üí FPR cao.
    
    Solution: Blend historical momentum (clean) v·ªõi current median (potentially corrupt)
    d·ª±a tr√™n H score.
    """
    
    def __init__(
        self,
        # Adaptive blending parameters
        alpha_base: float = 0.2,      # Base weight cho historical
        alpha_h_mult: float = 0.5,    # H multiplier: Œ± = Œ±_base + Œ±_h_mult √ó H
        alpha_min: float = 0.1,       # Min historical weight (lu√¥n gi·ªØ 1 √≠t historical)
        alpha_max: float = 0.8,       # Max historical weight (kh√¥ng b·ªè h·∫øt current)
        
        # Momentum parameters
        momentum_decay: float = 0.9,  # EMA decay cho momentum update
        
        # Safety parameters
        warmup_rounds: int = 10,      # S·ªë round warmup (ch·ªâ d√πng current)
        min_history_rounds: int = 3,  # S·ªë round t·ªëi thi·ªÉu ƒë·ªÉ d√πng historical
        
        # Round-based adaptation
        round_decay_start: int = 30,  # Round b·∫Øt ƒë·∫ßu gi·∫£m historical weight
        round_decay_factor: float = 0.01,  # M·ªói round sau 30, gi·∫£m Œ± th√™m factor
    ):
        """
        Initialize Adaptive Reference Tracker.
        
        Args:
            alpha_base: Base weight cho historical reference
            alpha_h_mult: Multiplier - tƒÉng historical weight khi H tƒÉng
            alpha_min: Minimum historical weight
            alpha_max: Maximum historical weight
            momentum_decay: EMA decay cho momentum update
            warmup_rounds: S·ªë round warmup (kh√¥ng d√πng historical)
            min_history_rounds: S·ªë round t·ªëi thi·ªÉu c·∫ßn thi·∫øt ƒë·ªÉ d√πng historical
            round_decay_start: Round b·∫Øt ƒë·∫ßu gi·∫£m historical weight
            round_decay_factor: Factor gi·∫£m Œ± m·ªói round sau round_decay_start
        """
        # Blending parameters
        self.alpha_base = alpha_base
        self.alpha_h_mult = alpha_h_mult
        self.alpha_min = alpha_min
        self.alpha_max = alpha_max
        
        # Momentum parameters
        self.momentum_decay = momentum_decay
        
        # Safety parameters
        self.warmup_rounds = warmup_rounds
        self.min_history_rounds = min_history_rounds
        self.round_decay_start = round_decay_start
        self.round_decay_factor = round_decay_factor
        
        # State
        self.historical_momentum: Optional[np.ndarray] = None
        self.update_count: int = 0
        self.last_alpha_used: float = 0.0
        self.last_h_score: float = 0.0
        
        # Logging
        self._log_init()
    
    def _log_init(self):
        """Log initialization parameters."""
        print(f"\n{'='*60}")
        print(f"‚úÖ AdaptiveReferenceTracker Initialized")
        print(f"{'='*60}")
        print(f"üìä Blending Formula: ref = Œ±√óhistorical + (1-Œ±)√ócurrent")
        print(f"   Œ± = clip(Œ±_base + Œ±_h_mult √ó H, Œ±_min, Œ±_max)")
        print(f"   Œ±_base={self.alpha_base}, Œ±_h_mult={self.alpha_h_mult}")
        print(f"   Œ±_min={self.alpha_min}, Œ±_max={self.alpha_max}")
        print(f"üìä Momentum decay: {self.momentum_decay}")
        print(f"üìä Safety: warmup={self.warmup_rounds}, min_history={self.min_history_rounds}")
        print(f"{'='*60}\n")
    
    # =========================================================================
    # CORE METHODS
    # =========================================================================
    
    def update_momentum(
        self,
        aggregated_gradient: np.ndarray,
        current_round: int,
        is_clean_round: bool = True
    ) -> None:
        """
        Update historical momentum v·ªõi aggregated gradient t·ª´ round v·ª´a xong.
        
        G·ªåI SAU KHI AGGREGATION XONG (cu·ªëi aggregate_fit).
        
        Args:
            aggregated_gradient: Gradient ƒë√£ aggregate (t·ª´ clean clients)
            current_round: Round hi·ªán t·∫°i
            is_clean_round: True n·∫øu round n√†y c√≥ √≠t attack/FP th·∫•p
                           (Warmup rounds lu√¥n l√† clean)
        
        Note:
            - Warmup rounds (1-10): Momentum ƒë∆∞·ª£c update v·ªõi weight cao h∆°n
            - Post-warmup: D√πng EMA decay b√¨nh th∆∞·ªùng
        """
        # Ensure flat array
        if isinstance(aggregated_gradient, list):
            flat_grad = np.concatenate([g.flatten() for g in aggregated_gradient])
        else:
            flat_grad = aggregated_gradient.flatten()
        
        # First update - initialize
        if self.historical_momentum is None:
            self.historical_momentum = flat_grad.copy()
            self.update_count = 1
            print(f"   üìä [AdaptiveRef] Initialized momentum (round {current_round})")
            return
        
        # Check dimension match
        if flat_grad.shape != self.historical_momentum.shape:
            print(f"   ‚ö†Ô∏è [AdaptiveRef] Shape mismatch! Reinitializing momentum.")
            self.historical_momentum = flat_grad.copy()
            self.update_count = 1
            return
        
        # EMA update
        # Trong warmup, d√πng decay cao h∆°n (h·ªçc nhanh t·ª´ clean data)
        if current_round <= self.warmup_rounds:
            decay = 0.7  # Higher weight cho new gradient trong warmup
        else:
            decay = self.momentum_decay
        
        # momentum = decay √ó old_momentum + (1-decay) √ó new_gradient
        self.historical_momentum = (
            decay * self.historical_momentum + 
            (1 - decay) * flat_grad
        )
        
        self.update_count += 1
        
        # Debug log m·ªói 10 rounds
        if current_round % 10 == 0 or current_round <= 3:
            momentum_norm = np.linalg.norm(self.historical_momentum)
            print(f"   üìä [AdaptiveRef] Momentum updated (round {current_round}, "
                  f"count={self.update_count}, norm={momentum_norm:.2f})")
    
    def compute_adaptive_reference(
        self,
        current_gradients: List[np.ndarray],
        h_score: float,
        current_round: int
    ) -> np.ndarray:
        """
        T√≠nh adaptive reference vector cho Layer 2.
        
        G·ªåI TR∆Ø·ªöC LAYER 2 DETECTION.
        
        Args:
            current_gradients: List gradients t·ª´ round hi·ªán t·∫°i
            h_score: Heterogeneity score t·ª´ NonIIDHandler
            current_round: Round hi·ªán t·∫°i
            
        Returns:
            reference: Blended reference vector
        """
        # Stack and compute current-round median
        grad_matrix = np.vstack([g.flatten() for g in current_gradients])
        current_median = np.median(grad_matrix, axis=0)
        
        # === CASE 1: Warmup phase - ch·ªâ d√πng current ===
        if current_round <= self.warmup_rounds:
            self.last_alpha_used = 0.0
            self.last_h_score = h_score
            print(f"   üõ°Ô∏è [AdaptiveRef] Warmup round {current_round}: Using CURRENT median only")
            return current_median
        
        # === CASE 2: Kh√¥ng ƒë·ªß history - ch·ªâ d√πng current ===
        if self.historical_momentum is None or self.update_count < self.min_history_rounds:
            self.last_alpha_used = 0.0
            self.last_h_score = h_score
            print(f"   üõ°Ô∏è [AdaptiveRef] Insufficient history ({self.update_count} rounds): "
                  f"Using CURRENT median only")
            return current_median
        
        # === CASE 3: Dimension mismatch - fallback to current ===
        if current_median.shape != self.historical_momentum.shape:
            self.last_alpha_used = 0.0
            self.last_h_score = h_score
            print(f"   ‚ö†Ô∏è [AdaptiveRef] Shape mismatch! Using CURRENT median only")
            return current_median
        
        # === CASE 4: Normal operation - adaptive blend ===
        
        # Compute adaptive alpha based on H score
        # Œ± = Œ±_base + Œ±_h_mult √ó H
        # Khi H cao (attack nhi·ªÅu) ‚Üí Œ± cao ‚Üí d√πng historical nhi·ªÅu h∆°n
        alpha = self.alpha_base + self.alpha_h_mult * h_score
        
        # Round-based decay (optional: gi·∫£m historical weight ·ªü late rounds)
        # V√¨ model ƒë√£ converge, current median ƒë√°ng tin h∆°n
        if current_round > self.round_decay_start:
            rounds_after_decay = current_round - self.round_decay_start
            alpha -= rounds_after_decay * self.round_decay_factor
        
        # Clip to valid range
        alpha = np.clip(alpha, self.alpha_min, self.alpha_max)
        
        # Blend: reference = Œ± √ó historical + (1-Œ±) √ó current
        reference = alpha * self.historical_momentum + (1 - alpha) * current_median
        
        # Store for logging
        self.last_alpha_used = alpha
        self.last_h_score = h_score
        
        # Log
        hist_weight_pct = alpha * 100
        curr_weight_pct = (1 - alpha) * 100
        print(f"   üõ°Ô∏è [AdaptiveRef] Round {current_round}: "
              f"H={h_score:.3f} ‚Üí Œ±={alpha:.3f} "
              f"(Historical: {hist_weight_pct:.1f}%, Current: {curr_weight_pct:.1f}%)")
        
        return reference
    
    # =========================================================================
    # UTILITY METHODS
    # =========================================================================
    
    def get_stats(self) -> Dict:
        """Get tracker statistics."""
        momentum_norm = (np.linalg.norm(self.historical_momentum) 
                        if self.historical_momentum is not None else 0.0)
        return {
            'update_count': self.update_count,
            'momentum_norm': float(momentum_norm),
            'last_alpha_used': float(self.last_alpha_used),
            'last_h_score': float(self.last_h_score),
            'has_momentum': self.historical_momentum is not None,
            'config': {
                'alpha_base': self.alpha_base,
                'alpha_h_mult': self.alpha_h_mult,
                'alpha_min': self.alpha_min,
                'alpha_max': self.alpha_max,
                'momentum_decay': self.momentum_decay,
                'warmup_rounds': self.warmup_rounds,
                'min_history_rounds': self.min_history_rounds
            }
        }
    
    def reset(self):
        """Reset tracker state."""
        self.historical_momentum = None
        self.update_count = 0
        self.last_alpha_used = 0.0
        self.last_h_score = 0.0
        print("   üîÑ [AdaptiveRef] Tracker reset")


# =============================================================================
# TESTING
# =============================================================================

def test_adaptive_reference_tracker():
    """Test AdaptiveReferenceTracker."""
    print("\n" + "="*70)
    print("üß™ TESTING ADAPTIVE REFERENCE TRACKER")
    print("="*70)
    
    np.random.seed(42)
    
    tracker = AdaptiveReferenceTracker(
        alpha_base=0.2,
        alpha_h_mult=0.5,
        alpha_min=0.1,
        alpha_max=0.8,
        warmup_rounds=3,
        min_history_rounds=2
    )
    
    # Simulate warmup
    print("\n--- Warmup Phase ---")
    for r in range(1, 4):
        grads = [np.random.randn(1000) for _ in range(10)]
        ref = tracker.compute_adaptive_reference(grads, h_score=0.1, current_round=r)
        agg = np.mean(np.vstack([g.flatten() for g in grads]), axis=0)
        tracker.update_momentum(agg, r)
        print(f"   Round {r}: Œ±={tracker.last_alpha_used:.3f}")
    
    # Simulate post-warmup with varying H scores
    print("\n--- Post-Warmup Phase ---")
    test_cases = [
        (4, 0.1, "Low H (IID)"),
        (5, 0.3, "Medium H"),
        (6, 0.6, "High H (attack likely)"),
        (7, 0.9, "Very High H (severe attack)"),
    ]
    
    for r, h, desc in test_cases:
        grads = [np.random.randn(1000) for _ in range(10)]
        ref = tracker.compute_adaptive_reference(grads, h_score=h, current_round=r)
        agg = np.mean(np.vstack([g.flatten() for g in grads]), axis=0)
        tracker.update_momentum(agg, r)
        print(f"   Round {r} ({desc}): H={h:.1f} ‚Üí Œ±={tracker.last_alpha_used:.3f}")
    
    # Verify alpha increases with H
    print("\n--- Verification ---")
    alphas = []
    for h in [0.0, 0.25, 0.5, 0.75, 1.0]:
        grads = [np.random.randn(1000) for _ in range(10)]
        tracker.compute_adaptive_reference(grads, h_score=h, current_round=10)
        alphas.append((h, tracker.last_alpha_used))
    
    is_monotonic = all(alphas[i][1] <= alphas[i+1][1] for i in range(len(alphas)-1))
    if is_monotonic:
        print("   ‚úÖ Alpha increases monotonically with H")
    else:
        print("   ‚ùå Alpha NOT monotonic with H")
        for h, a in alphas:
            print(f"      H={h:.2f} ‚Üí Œ±={a:.3f}")
    
    print("\n" + "="*70)
    print("‚úÖ TEST COMPLETED")
    print("="*70 + "\n")


if __name__ == "__main__":
    test_adaptive_reference_tracker()